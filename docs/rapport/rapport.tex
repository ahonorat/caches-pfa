\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{geometry}
\geometry{hmargin=3.5cm,vmargin=3.5cm}
\usepackage{graphicx}
\usepackage[nottoc, notlof, notlot]{tocbibind}

%% \usepackage[vlined, lined, linesnumbered, boxruled, french]{algorithm2e}
\usepackage{fancyhdr}
\pagestyle{fancy}

\lhead{Background}
\rhead{Simulateur de caches multi-c\oe ur}
\lfoot{ENSEIRB-MATMECA}
\rfoot{PFA 2013-2014}

\begin{document}

\input{pageGarde.tex}

\newpage
\tableofcontents

\newpage
\section*{Introduction}

\indent Dernièrement la vitesse des processeurs a considérablement augmenté (loi de Moore) alors que le temps d'accès à la mémoire RAM (Random-Access Memory) est resté globalement le même. Pour permettre d'accèder rapidement à des éléments mémoire non contenus dans les registres, des caches sont utilisés. Cette organisation hiérarchique de la mémoire a plusieurs objectifs: \\
\begin{itemize}
\item \^Etre assez conséquente en termes de taille pour pouvoir contenu la totalité de l'espace adressable.
\item \^Etre organisée de manière à être rapide.
\item Ne pas coûter trop cher. \\
\end{itemize}

\indent Les caches permettent de stocker la mémoire utilisée récemment dans les registres, en se basant sur deux concepts: la localité spatiale et la localité temporelle. La localité temporelle stipule qu'une cellule mémoire accédée récemment sera très probablement utilisée dans un futur proche. La localité spatiale est l'idée que si l'on accède à une cellule mémoire $X$, la cellule mémoire $X+1$ a de grandes chances d'être utilisée. \\

\indent Les mémoires de haut niveau, proches du processeur, sont généralement de petite taille. Leur coût est conséquent mais leur accès est très rapide. On peut résumer comme suit une hiérarchie mémoire classique: \\

\begin{figure}[!h]
\begin{center}
   \includegraphics[scale=0.75]{hierarchy.png}
   \caption{\label{hierarchy} Hiérachie mémoire}
\end{center}
\end{figure}

\indent Le but de ce document est de résumer un certain nombre de techniques relatives à la bonne gestion de cette hiérarchie, en se focalisant sur les caches. Nous commencerons par expliciter le comportement d'un cache en général, avant d'étudier les moyens mis en {\oe}uvre afin d'assurer la cohérence de l'ensemble des caches. Nous finirons par proposer quelques algorithmes permettant de simuler de manière la plus générique possible le comportement des caches.

\newpage
\section{Fonctionnement d'un cache}
\indent Cette partie entend préciser le fonctionnement général d'un cache: comment il est possible d'y ajouter une donnée, quelle est la correspondance entre les blocs mémoires et les lignes de cache ou encore comment une donnée peut être evincée d'un cache.

\subsection{\'Etiquettes}
\indent Quand un bloc mémoire (généralement $1$ octet) est ajouté dans le cache, la ligne entière correspondante ($32$ ou $64$ octets) est chargée dans le cache, afin d'exploiter le principe de localité spatiale et de faire du prefetching de données. Pour pouvoir retrouver une donnée dans le cache, une table d'étiquettes est tenue à jour. Concrètement, une adresse mémoire est séparée en trois champs comme sur la figure suivante: \\

\begin{figure}[!h]
\begin{center}
   \includegraphics[scale=0.50]{etiquette.jpeg}
   \caption{\label{etiquette} Adresse mémoire}
\end{center}
\end{figure}

\indent Le tag est stocké dans la table des étiquettes, il servira a identifier les différents blocs mémoires pouvant être au même endroit dans un cache. L'index correspond au numéro de set dans lequel se trouve la ligne de cache. Pour finir l'offset correspond au bloc dans la ligne de cache.

\subsection{Fonction de correspondance}
\indent Un cache de taille $n$ contient un ensemble $p$ de lignes de taille $m$, tels que $n = p \times m$. Afin de placer et récupérer une donnée dans le cache, une fonction de correspondance avec la mémoire est nécessaire. Il existe trois cas de figure. Pour la suite, nous prendrons: \\
\begin{itemize}
\item $i$ le numéro de set du cache
\item $j$ le numéro du bloc mémoire
\item $s$ le nombre de sets du cache
\item $k$ l'associativité du cache 
\end{itemize}

\subsubsection{Direct associative}
\indent Un cache est en correspondance directe si à chaque bloc mémoire est associé une unique ligne du cache. Le nombre de sets, $s$, du cache est alors égal à son nombre de lignes, $p$. Lorsqu'un bloc mémoire $j$ est ajouté dans le cache, la ligne correpondante est chargée à la ligne $i = j\ modulo\ s$. Avec ce type de cache, il est facile d'ajouter ou de retrouver une données. Cependant, si plusieurs blocs mémoires correpondant à la même ligne de cache sont fréquemment utilisés, il faudra sans cesse supprimer et ajouter des données dans le cache.

\subsubsection{Fully associative}
\indent Un cache est en correspondance associative si chaque bloc mémoire peut être mis dans n'importe quelle ligne du cache. Il n'y a alors qu'un seul set. L'inconvénient précédent n'est plus existant, cependant il devient beaucoup plus compliqué de rechercher une donnée dans le cache. L'ensemble des tags doit en effet être parcouru.

\subsubsection{$k$-ways associative}
\indent Les deux cas présentés précedemment présentent des inconvénients. Généralement, un cache profite des avantages des deux visions en faisant un compromis. Dans le cas de la correspondance associative par ensemble, chaque set possède un nombre $k$ de lignes, appelé associativité du cache et tel que $p=k \times s$. La fonction de correspondance est telle que $i = j\ modulo\ s$. De cette manière, un bloc mémoire peut se trouver dans un ensemble de $k$ lignes. Il est donc possible d'avoir plusieurs blocs mémoires correspondant au même ensemble sans trop perdre en performances et l'algorithme de recherche est plus efficace que dans le cas de la correspondance associative.

\begin{figure}[!h]
\begin{center}
   \includegraphics[scale=0.60]{associative.png}
   \caption{\label{associative} Fonction de correspondance}
\end{center}
\end{figure}

\subsection{Politiques de remplacement}
\indent Lorsque l'on souhaite ajouter une ligne dans un set plein, il faut au préalable evincer une ligne de ce set. Pour cela, il existe différentes méthodes:

\paragraph{FIFO.} La première solution consiste à supprimer la ligne la plus ancienne du cache. Cela correspond à la formule: ``First In, First Out''.

\paragraph{LFU.} Une autre solution consiste à supprimer la ligne qui a été le moins utilisée: ``Least Frequently Used''. Pour cela, chaque ligne de cache possède un compteur qui sera incrémenté à chaque utilisation de la ligne. 

\paragraph{LRU.} La dernière solution, généralement utilisée, consiste à favoriser le principe de localité temporelle en supprimant la ligne du set qui a la plus ancienne date d'utilisation: ``Least Recently Used''.

\newpage
\section{Gestion de la cohérence}
\indent Dans les systèmes actuels, un processeur n'est plus composé d'un unique c{\oe}ur, mais de plusieurs. Chaque c{\oe}ur possède généralement deux caches de plus haut niveau: le L1i pour stocker les instructions et le L1d pour stocker les données. Si les interactions entre ces deux caches sont minimes, il est difficile de mélanger instructions et données, la cohérence entre les différents c{\oe}ur est un problème de taille.

\subsection{Présentation du problème}
\indent Les caches sont utilisés à chaque load/store. Si il est possible que plusieurs caches possèdent la même donnée et la lisent en même temps, il est primordial de définir un protocole de cohérence afin qu'une donnée ne puisse pas être modifiée simultanément dans deux caches. Pour cela, des techniques hardwares sont mises en place afin de définir qui a la priorité si deux c{\oe}urs veulent modifier une même donnée. \\

\indent Par ailleurs, un protocole de cohérence est mis en {\oe}uvre à chaque load/store afin que les différents caches soit informés des modifications les concernant et que la consistance du système soit assurée. Ce protocole est propre à un niveau de cache. Dans un cas classique, il y aura un protocole de cohérence entre les L1 et entre les L2.

\subsection{Protocoles de cohérence}
\indent Nous étudierons uniquement le protocole de type MSI et ses dérivés: MESI, MOSI et MOESI. Chaque ligne de cache possède un état qui permet de gérer la cohérence. Les différents états sont: \\
\begin{itemize}
\item M: Une ligne est dans l'état modifié si c'est la seule copie valide dans l'ensemble des caches du niveau. Dans ce cas, si la ligne est evincée du cache, elle doit être recopiée en mémoire, via un write back. \\
\item S: Une ligne est dans l'état shared si elle est valide et qu'elle n'a pas été modifiée. Dans ce cas, plusieurs caches peuvent possèder la ligne. \\
\item I: L'état invalide est utilisé pour une ligne qui n'est pas valide. Le contenu de la ligne n'est pas viable, il ne faut pas l'utiliser. \\
\item E: Une ligne est dite exclusive si c'est, dans le niveau, la seule copie valide. Une ligne dans cet état n'a pas été modifiée et les données de la ligne sont identiques à celles de la mémoire principale. \\
\item O: L'état owned est utilisé pour un cache qui possède une donnée invalide dans la mémoire principale. Plusieurs caches peuvent possèder la même donnée, ils seront alors dans l'état S. \\
\end{itemize}

\indent L'état M est utilisée lorsque la donnée a été modifiée par un c{\oe}ur. Il existe deux cas de propagation des modifications. Dans le choix de la politique Write-Through, la donnée est directement recopiée dans la mémoire principale pour éviter de futurs problèmes de cohérence. Dans le cas de la politique Write-Back, la donnée est modifiée uniquement dans le cache. Les autres caches et la mémoire principale peuvent savoir que la donnée a été modifiée, en revanche ils n'ont pas la dernière copie valide. Ils peuvent l'obtenir lors des Write-Backs: lorsque la donnée est evincée du cache ou lorsqu'elle est demandée à un plus haut niveau pour des soucis de cohérence.

\newpage
\section{Fonctionnement global}

\subsection{Caches inclusifs}

\subsection{Caches exclusifs}

\subsection{Caches non-inclusifs}


\newpage
\section{Comportements spéciaux}

\subsection{Utilisation d'un victim cache}

\subsection{Tracking}
%L3 sait qui a les données
%NUCA

\newpage
\section{Algorithmes de simulation}

\subsection{Interaction entre les différents niveaux de caches}

\subsection{Gestion des différentes modularités}

\newpage
\nocite{*}
\bibliographystyle{plain}
\bibliography{rapport}

\end{document}
